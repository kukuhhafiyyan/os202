__WEEK 06__

1. [Preemptive and Non-Preemptive Scheduling](https://www.tutorialspoint.com/preemptive-and-non-preemptive-scheduling#:~:text=Preemptive%20Scheduling%20is%20a%20CPU,CPU%20to%20a%20given%20process.&text=Non%2Dpreemptive%20Scheduling%20is%20a,pushed%20to%20the%20waiting%20state.)

	Preemptive Scheduling is a CPU scheduling technique that works by dividing time slots of CPU to a given process. The time slot given might be able to complete the whole process or might not be able to it. When the burst time of the process is greater than CPU cycle, it is placed back into the ready queue and will execute in the next chance. This scheduling is used when the process switch to ready state. Non-preemptive Scheduling is a CPU scheduling technique the process takes the resource (CPU time) and holds it till the process gets terminated or is pushed to the waiting state. No process is interrupted until it is completed, and after that processor switches to another process.

2. [Asymmetric Multiprocessing](https://www.tutorialspoint.com/Asymmetric-Multiprocessing)

	Asymmetric multiprocessor systems are a part of multiprocessor systems along with symmetric multiprocessor systems. Multiprocessor systems have multiple processors working in parallel that share the computer clock, memory, bus, peripheral devices etc.

3. [Symmetric Multiprocessing](https://www.tutorialspoint.com/Symmetric-Multiprocessing)

	In symmetric multiprocessing, multiple processors share a common memory and operating system. All of these processors work in tandem to execute processes. The operating system treats all the processors equally, and no processor is reserved for special purposes.

4. [Load Balancing](https://www.citrix.com/en-in/glossary/load-balancing.html#:~:text=Load%20balancing%20is%20defined%20as,server%20capable%20of%20fulfilling%20them.)

	Load balancing is defined as the methodical and efficient distribution of network or application traffic across multiple servers in a server farm. Each load balancer sits between client devices and backend servers, receiving and then distributing incoming requests to any available server capable of fulfilling them.

5. [Multicore Processor](https://www.computerhope.com/jargon/m/multcore.htm#:~:text=A%20multicore%20processor%20is%20a,advantage%20of%20the%20unique%20architecture.)

	A multicore processor is a single computing component comprised of two or more CPUs that read and execute the actual program instructions. The individual cores can execute multiple instructions in parallel, increasing the performance of software which is written to take advantage of the unique architecture.

6. I/O Bound Process
	
	Processes that are mostly waiting for the completion of input or output (I/O) are I/O Bound. Interactive processes, such as office applications are mostly I/O bound the entire life of the process. Some processes may be I/O bound for only a few short periods of time. The expected short run time of I/O bound processes means that they will not stay the running the process for very long. They should be given high priority by the scheduler.

7. CPU Bound Process

	CPU Bound processes are ones that are implementing algorithms with a large number of calculations. They can be expected to hold the CPU for as long as the scheduler will allow. Programs such as simulations may be CPU bound for most of the life of the process. Users do not typically expect an immediate response from the computer when running CPU bound programs. They should be given a lower priority by the scheduler.

8. [Processor Affinity](https://www.thewindowsclub.com/processor-affinity-windows)

	Processor Affinity also called CPU pinning, allows the user to assign a process to use only a few cores. Technically you can bind and unbind a process or thread to CPU or CPUs which here can be termed as CPU cores. But the real question is why is such an option available, and is there an advantage of setting processor affinity. Processor Affinity is useful if you have a heavy program like video rendering. When you dedicated a core for the video editing program, it ensures that the core of the processor is always dedicated to the task. It improves performance because it reduces reduce cache problem as there is no delay with a dedicated core.

9. [Big O Notation](https://www.freecodecamp.org/news/big-o-notation-why-it-matters-and-why-it-doesnt-1674cfa8a23c/)

	Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmannâ€“Landau notation or asymptotic notation. In plain words, Big O notation describes the complexity of your code using algebraic terms.

10. [Process and Thread](https://www.geeksforgeeks.org/difference-between-process-and-thread/)

	Process means any program is in execution. Process control block controls the operation of any process. Process control block contains information about processes for example Process priority, process id, process state, CPU, register, etc. A process can creates other processes which are known as Child Processes. Process takes more time to terminate and it is isolated means it does not share memory with any other process. Thread is the segment of a process means a process can have multiple threads and these multiple threads are contained within a process. A thread have 3 states: running, ready, and blocked.
